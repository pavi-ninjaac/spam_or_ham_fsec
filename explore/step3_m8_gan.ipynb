{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, LeakyReLU\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.read_csv(\"/home/pavithra/projects/spam_or_ham_fsec/data/source/train_data.csv\", header=None)\n",
    "# data.head()\n",
    "# target = pd.read_csv(\"/home/pavithra/projects/spam_or_ham_fsec/data/source/train_labels.csv\", header=None)\n",
    "# target.head()\n",
    "\n",
    "# # Did the simple preprocessing without sampling. do PCA and target data changes.\n",
    "# X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.1, random_state=42, stratify=target)\n",
    "\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# X_train_scaled = scaler.fit_transform(X_train)\n",
    "# X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# n_components = 2000\n",
    "# pca = PCA(n_components=n_components)\n",
    "\n",
    "# X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "# X_test_pca = pca.transform(X_test_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train.replace(1, 0, inplace=True)\n",
    "# y_train.replace(-1, 1, inplace=True)\n",
    "\n",
    "\n",
    "# y_test.replace(1, 0, inplace=True)\n",
    "# y_test.replace(-1, 1, inplace=True)\n",
    "\n",
    "# np.savetxt(\"data/x_train.csv\", X_train_pca, delimiter=',')\n",
    "# np.savetxt(\"data/x_test.csv\", X_test_pca, delimiter=',')\n",
    "# np.savetxt(\"data/y_train.csv\", y_train, delimiter=',', fmt='%d')\n",
    "# np.savetxt(\"data/y_test.csv\", y_test, delimiter=',', fmt='%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X train  (3375, 2000)\n",
      "X test (375, 2000)\n",
      "Y_Train (3375, 1)\n",
      "Y_test (375, 1)\n"
     ]
    }
   ],
   "source": [
    "# Load the data from the pre-processed data.\n",
    "X_train = pd.read_csv('data/x_train.csv', header=None)\n",
    "X_test = pd.read_csv('data/x_test.csv', header=None)\n",
    "y_train = pd.read_csv('data/y_train.csv', header=None)\n",
    "y_test = pd.read_csv('data/y_test.csv', header=None)\n",
    "\n",
    "print(\"X train \",X_train.shape)\n",
    "print(\"X test\",X_test.shape)\n",
    "print(\"Y_Train\", y_train.shape)\n",
    "print(\"Y_test\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0\n",
       "0    0.900148\n",
       "1    0.099852\n",
       "Name: count, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the miniority class.\n",
    "y_train.value_counts() / y_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into majarity class and minority class.\n",
    "merged_data = pd.concat([X_train, y_train], axis=1)\n",
    "not_spam_data = merged_data[merged_data.iloc[:, -1] == 0]\n",
    "spam_data = merged_data[merged_data.iloc[: , -1] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0\n",
       "0    3038\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_spam_data.iloc[:, -1].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0\n",
       "1    337\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_data.iloc[:, -1].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_data = spam_data.iloc[:, :-1]\n",
    "spam_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(337, 2000)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Assume minority_class_embeddings is the minority class data (shape: [num_samples, embedding_dim])\n",
    "minority_class_embeddings = spam_data\n",
    "embedding_dim = minority_class_embeddings.shape[1]\n",
    "\n",
    "# Define the Generator\n",
    "def build_generator(noise_dim, embedding_dim):\n",
    "    model = Sequential([\n",
    "        Dense(256, input_dim=noise_dim),\n",
    "        LeakyReLU(alpha=0.2),\n",
    "        Dense(512),\n",
    "        LeakyReLU(alpha=0.2),\n",
    "        Dense(embedding_dim, activation='linear'),\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Define the Discriminator\n",
    "def build_discriminator(embedding_dim):\n",
    "    model = Sequential([\n",
    "        Dense(512, input_dim=embedding_dim),\n",
    "        LeakyReLU(alpha=0.2),\n",
    "        Dense(256),\n",
    "        LeakyReLU(alpha=0.2),\n",
    "        Dense(1, activation='sigmoid'),\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Hyperparameters\n",
    "noise_dim = 2000\n",
    "batch_size = 64\n",
    "epochs = 10\n",
    "learning_rate = 0.0002\n",
    "\n",
    "# Build and compile the discriminator\n",
    "discriminator = build_discriminator(embedding_dim)\n",
    "discriminator.compile(loss=keras.losses.BinaryCrossentropy(), optimizer=Adam(learning_rate), metrics=['accuracy'])\n",
    "\n",
    "# Build the generator\n",
    "generator = build_generator(noise_dim, embedding_dim)\n",
    "\n",
    "# Build and compile the GAN\n",
    "discriminator.trainable = False  # Freeze the discriminator during generator training\n",
    "gan_input = tf.keras.Input(shape=(noise_dim,))\n",
    "gan_output = discriminator(generator(gan_input))\n",
    "gan = tf.keras.Model(gan_input, gan_output)\n",
    "gan.compile(loss=keras.losses.BinaryCrossentropy(), optimizer=Adam(learning_rate))\n",
    "\n",
    "# Training the GAN\n",
    "for epoch in range(epochs):\n",
    "    for _ in range(len(minority_class_embeddings) // batch_size):\n",
    "        # Train Discriminator\n",
    "        idx = np.random.randint(0, minority_class_embeddings.shape[0], batch_size)\n",
    "        real_embeddings = minority_class_embeddings.iloc[idx, :]\n",
    "        real_labels = np.ones((batch_size, 1))  # Label 1 for real data\n",
    "\n",
    "        noise = np.random.normal(0, 1, (batch_size, noise_dim))\n",
    "        fake_embeddings = generator.predict(noise)\n",
    "        fake_labels = np.zeros((batch_size, 1))  # Label 0 for fake data\n",
    "\n",
    "        # Combine real and fake data\n",
    "        combined_embeddings = np.vstack([real_embeddings, fake_embeddings])\n",
    "        combined_labels = np.vstack([real_labels, fake_labels])\n",
    "\n",
    "        # Train the discriminator\n",
    "        loss_tracker = tf.keras.metrics.Mean(name='loss')\n",
    "        d_loss = discriminator.train_on_batch(combined_embeddings, combined_labels)\n",
    "\n",
    "        # Train Generator\n",
    "        noise = np.random.normal(0, 1, (batch_size, noise_dim))\n",
    "        valid_labels = np.ones((batch_size, 1))  # Label 1 for fooling the discriminator\n",
    "        g_loss = gan.train_on_batch(noise, valid_labels)\n",
    "\n",
    "    # Logging\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch + 1}/{epochs} | D Loss: {d_loss[0]:.4f}, Acc: {d_loss[1]*100:.2f}% | G Loss: {g_loss:.4f}\")\n",
    "\n",
    "# Generate synthetic embeddings\n",
    "def generate_synthetic_data(generator, num_samples, noise_dim):\n",
    "    noise = np.random.normal(0, 1, (num_samples, noise_dim))\n",
    "    synthetic_data = generator.predict(noise)\n",
    "    return synthetic_data\n",
    "\n",
    "# Generate new minority class samples\n",
    "num_new_samples = 500  # Number of synthetic samples to generate\n",
    "synthetic_embeddings = generate_synthetic_data(generator, num_new_samples, noise_dim)\n",
    "\n",
    "print(\"Synthetic embeddings generated with shape:\", synthetic_embeddings.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.backend import clear_session\n",
    "\n",
    "clear_session()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spam_ham_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
