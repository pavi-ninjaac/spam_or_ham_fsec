{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, LeakyReLU\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Assume minority_class_embeddings is the minority class data (shape: [num_samples, embedding_dim])\n",
    "minority_class_embeddings = np.random.rand(300, 10000)  # Example minority class embeddings\n",
    "embedding_dim = minority_class_embeddings.shape[1]\n",
    "\n",
    "# Define the Generator\n",
    "def build_generator(noise_dim, embedding_dim):\n",
    "    model = Sequential([\n",
    "        Dense(256, input_dim=noise_dim),\n",
    "        LeakyReLU(alpha=0.2),\n",
    "        Dense(512),\n",
    "        LeakyReLU(alpha=0.2),\n",
    "        Dense(embedding_dim, activation='linear'),\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Define the Discriminator\n",
    "def build_discriminator(embedding_dim):\n",
    "    model = Sequential([\n",
    "        Dense(512, input_dim=embedding_dim),\n",
    "        LeakyReLU(alpha=0.2),\n",
    "        Dense(256),\n",
    "        LeakyReLU(alpha=0.2),\n",
    "        Dense(1, activation='sigmoid'),\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Hyperparameters\n",
    "noise_dim = 100  # Size of the random noise vector\n",
    "batch_size = 64\n",
    "epochs = 100\n",
    "learning_rate = 0.0002\n",
    "\n",
    "# Build and compile the discriminator\n",
    "discriminator = build_discriminator(embedding_dim)\n",
    "discriminator.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate), metrics=['accuracy'])\n",
    "\n",
    "# Build the generator\n",
    "generator = build_generator(noise_dim, embedding_dim)\n",
    "\n",
    "# Build and compile the GAN\n",
    "discriminator.trainable = False  # Freeze the discriminator during generator training\n",
    "gan_input = tf.keras.Input(shape=(noise_dim,))\n",
    "gan_output = discriminator(generator(gan_input))\n",
    "gan = tf.keras.Model(gan_input, gan_output)\n",
    "gan.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate))\n",
    "\n",
    "# Training the GAN\n",
    "for epoch in range(epochs):\n",
    "    for _ in range(len(minority_class_embeddings) // batch_size):\n",
    "        # Train Discriminator\n",
    "        idx = np.random.randint(0, minority_class_embeddings.shape[0], batch_size)\n",
    "        real_embeddings = minority_class_embeddings[idx]\n",
    "        real_labels = np.ones((batch_size, 1))  # Label 1 for real data\n",
    "\n",
    "        noise = np.random.normal(0, 1, (batch_size, noise_dim))\n",
    "        fake_embeddings = generator.predict(noise)\n",
    "        fake_labels = np.zeros((batch_size, 1))  # Label 0 for fake data\n",
    "\n",
    "        # Combine real and fake data\n",
    "        combined_embeddings = np.vstack([real_embeddings, fake_embeddings])\n",
    "        combined_labels = np.vstack([real_labels, fake_labels])\n",
    "\n",
    "        # Train the discriminator\n",
    "        d_loss = discriminator.train_on_batch(combined_embeddings, combined_labels)\n",
    "\n",
    "        # Train Generator\n",
    "        noise = np.random.normal(0, 1, (batch_size, noise_dim))\n",
    "        valid_labels = np.ones((batch_size, 1))  # Label 1 for fooling the discriminator\n",
    "        g_loss = gan.train_on_batch(noise, valid_labels)\n",
    "\n",
    "    # Logging\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch + 1}/{epochs} | D Loss: {d_loss[0]:.4f}, Acc: {d_loss[1]*100:.2f}% | G Loss: {g_loss:.4f}\")\n",
    "\n",
    "# Generate synthetic embeddings\n",
    "def generate_synthetic_data(generator, num_samples, noise_dim):\n",
    "    noise = np.random.normal(0, 1, (num_samples, noise_dim))\n",
    "    synthetic_data = generator.predict(noise)\n",
    "    return synthetic_data\n",
    "\n",
    "# Generate new minority class samples\n",
    "num_new_samples = 500  # Number of synthetic samples to generate\n",
    "synthetic_embeddings = generate_synthetic_data(generator, num_new_samples, noise_dim)\n",
    "\n",
    "print(\"Synthetic embeddings generated with shape:\", synthetic_embeddings.shape)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
